{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week14 Reinforcemnet Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning in a grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from environment import Env\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.actions = actions        # 0, 1, 2, or 3 (up, down, left, right)\n",
    "        self.learning_rate = alpha    # alpha\n",
    "        self.discount_factor = gamma  # gamma\n",
    "        self.epsilon = epsilon        # ratio of exploration\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # select action in current state - epsilon exploration\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # select random action - exploration\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # select action a with best Q(s,a) - exploitation\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "\n",
    "    # 1 step Q-learning - update Q(s,a) from <s,a,r,s'>\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Q(s,a)\n",
    "        q_1 = self.q_table[state][action]\n",
    "        # r + discount_factor * max(Q(s',a))\n",
    "        q_2 = reward+self.discount_factor*max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.learning_rate*(q_2-q_1)\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Exploration (ùúÄ=0)\n",
    "- ùõæ = 0.9\n",
    "- ùõº = 1 \n",
    "- ùúÄ = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f783a2f73037>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Q-learning through episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# initial state [0, 0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ML\\Week14\\environment.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords_to_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TF\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2469\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m         \u001b[1;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "agent = QLearningAgent(actions=list(range(env.n_actions)),\n",
    "                       gamma=0.9,\n",
    "                       alpha=1,  # Totally replace the q-func value with the current value\n",
    "                       epsilon=0 # No exploration\n",
    "                      ) \n",
    "n_episode = 100\n",
    "\n",
    "# Q-learning through episodes\n",
    "for episode in range(n_episode):\n",
    "    state = env.reset()     # initial state [0, 0]\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        \n",
    "        # select action a in current state s\n",
    "        action = agent.get_action(str(state))    \n",
    "        \n",
    "        # get next state s', reward r, done (True if it reached the final state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # update Q(s,a) from <s,a,r,s'>\n",
    "        agent.learn(str(state), action, reward, str(next_state))\n",
    "        state = next_state\n",
    "\n",
    "        # display all Q-function values\n",
    "        env.print_value_all(agent.q_table)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% Exploration (ùúÄ = 0.5)\n",
    "- ùõæ = 0.9\n",
    "- ùõº = 1\n",
    "- ùúÄ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1db71c3b379e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# get next state s', reward r, done (True if it reached the final state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# update Q(s,a) from <s,a,r,s'>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ML\\Week14\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mbase_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TF\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2469\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m         \u001b[1;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "agent = QLearningAgent(actions=list(range(env.n_actions)),\n",
    "                       gamma=0.9,\n",
    "                       alpha=1, \n",
    "                       epsilon=0.5\n",
    "                      ) \n",
    "n_episode = 100\n",
    "\n",
    "# Q-learning through episodes\n",
    "for episode in range(n_episode):\n",
    "    state = env.reset()     # initial state [0, 0]\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        \n",
    "        # select action a in current state s\n",
    "        action = agent.get_action(str(state))    \n",
    "        \n",
    "        # get next state s', reward r, done (True if it reached the final state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # update Q(s,a) from <s,a,r,s'>\n",
    "        agent.learn(str(state), action, reward, str(next_state))\n",
    "        state = next_state\n",
    "\n",
    "        # display all Q-function values\n",
    "        env.print_value_all(agent.q_table)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate (ùõº) \n",
    "It means the ratio of the q-func value to be updated with the newly computed q-func value.  \n",
    "If ùõº=1, then the q-func value will totally be updated with the newly computed q-func value \n",
    "- ùõæ = 0.9\n",
    "- ùõº = 0.1 \n",
    "- ùúÄ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f778fefeea63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Q-learning through episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# initial state [0, 0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ML\\Week14\\environment.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords_to_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\TF\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mcoords\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2467\u001b[0m         return [self.tk.getdouble(x) for x in\n\u001b[0;32m   2468\u001b[0m                            self.tk.splitlist(\n\u001b[1;32m-> 2469\u001b[1;33m                    self.tk.call((self._w, 'coords') + args))]\n\u001b[0m\u001b[0;32m   2470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Args: (val, val, ..., cnf={})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m         \u001b[1;34m\"\"\"Internal function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "agent = QLearningAgent(actions=list(range(env.n_actions)),\n",
    "                       gamma=0.9,\n",
    "                       alpha=0.1, \n",
    "                       epsilon=0.1\n",
    "                      )\n",
    "n_episode = 100\n",
    "\n",
    "# Q-learning through episodes\n",
    "for episode in range(n_episode):\n",
    "    state = env.reset()     # initial state [0, 0]\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        \n",
    "        # select action a in current state s\n",
    "        action = agent.get_action(str(state))    \n",
    "        \n",
    "        # get next state s', reward r, done (True if it reached the final state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # update Q(s,a) from <s,a,r,s'>\n",
    "        agent.learn(str(state), action, reward, str(next_state))\n",
    "        state = next_state\n",
    "\n",
    "        # display all Q-function values\n",
    "        env.print_value_all(agent.q_table)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Q-Network CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-a80792520bd0>\", line 9, in <module>\n",
      "    import gym\n",
      "ModuleNotFoundError: No module named 'gym'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-a80792520bd0>\", line 9, in <module>\n",
      "    import gym\n",
      "ModuleNotFoundError: No module named 'gym'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3454, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-a80792520bd0>\", line 9, in <module>\n",
      "    import gym\n",
      "ModuleNotFoundError: No module named 'gym'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3454, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3376, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"E:\\Anaconda3\\envs\\TF\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import random\n",
    "from collections import deque\n",
    "import dqn\n",
    "\n",
    "import gym\n",
    "from typing import List\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "#env = gym.wrappers.Monitor(env, directory=\"gym-results/\", force=True)\n",
    "\n",
    "# Constants defining our neural network\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE_FREQUENCY = 5\n",
    "MAX_EPISODES = 5000\n",
    "\n",
    "\n",
    "def replay_train(mainDQN: dqn.DQN, targetDQN: dqn.DQN, train_batch: list) -> float:\n",
    "    \"\"\"Trains `mainDQN` with target Q values given by `targetDQN`\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): Main DQN that will be trained\n",
    "        targetDQN (dqn.DQN): Target DQN that will predict Q_target\n",
    "        train_batch (list): Minibatch of replay memory\n",
    "            Each element is (s, a, r, s', done)\n",
    "            [(state, action, reward, next_state, done), ...]\n",
    "    Returns:\n",
    "        float: After updating `mainDQN`, it returns a `loss`\n",
    "    \"\"\"\n",
    "    states = np.vstack([x[0] for x in train_batch])\n",
    "    actions = np.array([x[1] for x in train_batch])\n",
    "    rewards = np.array([x[2] for x in train_batch])\n",
    "    next_states = np.vstack([x[3] for x in train_batch])\n",
    "    done = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X = states\n",
    "\n",
    "    # target\n",
    "    Q_target = rewards + DISCOUNT_RATE * np.max(targetDQN.predict(next_states), axis=1) * ~done\n",
    "\n",
    "    # prediction\n",
    "    y = mainDQN.predict(states)\n",
    "    y[np.arange(len(X)), actions] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    return mainDQN.update(X, y)\n",
    "\n",
    "\n",
    "def get_copy_var_ops(*, dest_scope_name: str, src_scope_name: str) -> List[tf.Operation]:\n",
    "    \"\"\"Creates TF operations that copy weights from `src_scope` to `dest_scope`\n",
    "    Args:\n",
    "        dest_scope_name (str): Destination weights (copy to)\n",
    "        src_scope_name (str): Source weight (copy from)\n",
    "    Returns:\n",
    "        List[tf.Operation]: Update operations are created and returned\n",
    "    \"\"\"\n",
    "    # Copy variables src_scope to dest_scope\n",
    "    op_holder = []\n",
    "    src_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=src_scope_name)\n",
    "    dest_vars = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=dest_scope_name)\n",
    "\n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        # Store the operation to update the main weight to the target weight in the op_holder list\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def bot_play(mainDQN: dqn.DQN, env: gym.Env) -> None:\n",
    "    \"\"\"Test runs with rendering and prints the total score\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN agent to run a test\n",
    "        env (gym.Env): Gym Environment\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(reward_sum))\n",
    "            break\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"main\")\n",
    "        targetDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE, name=\"target\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initial copy q_net -> target_net\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\",\n",
    "                                    src_scope_name=\"main\")\n",
    "        # Run update operations\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            \n",
    "            # epsilon decay\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            score = 0\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                env.render()\n",
    "\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Choose an action by greedily from the Q-network\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                # Get new state and reward from environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:  # Penalty\n",
    "                    reward = -1\n",
    "\n",
    "                # Save the experience to our buffer\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "\n",
    "                # Update main to target every 5 times\n",
    "                if step_count % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    sess.run(copy_ops)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "            print(\"Episode: {} steps: {} reward: {}\".format(episode, step_count, score))\n",
    "\n",
    "            # CartPole-v0 Game Clear Checking Logic\n",
    "            last_100_game_reward.append(score)\n",
    "\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "\n",
    "                if avg_reward > 199:\n",
    "                    print(\"Game Cleared in {episode} episodes with avg reward {avg_reward}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
